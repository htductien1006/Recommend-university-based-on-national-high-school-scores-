{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init('/home/ductien/yspark-3.3.2-bin-hadoop3')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "                    .config(\"spark.jars\", \"./mysql-connector-java-8.0.29.jar\")\\\n",
    "                    .appName(\"MySQL_loader\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_2017_df = spark.read.csv(\"./CleanedDatasets/Mark2017\", header= True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_18_22_df = spark.read.csv(\"./CleanedDatasets/Mark_2018_2022\", header= True, inferSchema= True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Spark dataframe to MySQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "dbName = os.getenv(\"DB_NAME\")\n",
    "dbUser = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASSWORD\")\n",
    "host = os.getenv(\"DB_HOST\")\n",
    "port = os.getenv(\"DB_PORT\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Load to province table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_2017_df.createTempView(\"PROVINCE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinceDF = spark.sql(\"SELECT DSTINCT province_code, province_name FROM PROVINCE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinceDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinceDF = provinceDF.sort(\"province_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinceDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newRow = spark.createDataFrame([(39,'phuyen')], provinceDF.columns)\n",
    "provinceDF = provinceDF.union(newRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newRow = spark.createDataFrame([(45,'ninhthuan')], provinceDF.columns)\n",
    "provinceDF = provinceDF.union(newRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinceDF = provinceDF.sort('province_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinceDF = provinceDF.withColumnRenamed('province', 'ProvinceName') \\\n",
    "                        .withColumnRenamed('province_code', 'provinceCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinceDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinceDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinceDF.write.format('jdbc').options(\n",
    "    url = 'jdbc:mysql://localhost:3306/equivalent_score',\n",
    "    driver = 'com.mysql.jdbc.Driver',\n",
    "    dbtable = \"DimProvince\",\n",
    "    user = dbUser,\n",
    "    password = password\n",
    ").mode('append').save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Load to Year table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearDF = spark.createDataFrame([\n",
    "    Row(year = 2018),\n",
    "    Row(year = 2019),\n",
    "    Row(year = 2020),\n",
    "    Row(year = 2021),\n",
    "    Row(year = 2022)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearDF.write.format('jdbc').options(\n",
    "    url = 'jdbc:mysql://localhost:3306/equivalent_score',\n",
    "    driver = 'com.mysql.jdbc.Driver',\n",
    "    dbtable = \"DimYear\",\n",
    "    user = dbUser,\n",
    "    password = password\n",
    ").mode('append').save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Load to DimUniverity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_df.createTempView('UNI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniDF = spark.sql(\"SELECT uni_code AS UniCode, uni_name AS UniName, \\\n",
    "                   major_code AS MajorCode, major_name AS MajorName, \\\n",
    "                   subject_group AS SubjectGroup, benchmark AS BenchMark, year AS Year FROM UNI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, when, isnan, isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniDF.select([count(when(isnan(c) | isnull(c), c)).alias(c) for c in uniDF.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniDF = uniDF.dropna(subset= uniDF.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniDF.select([count(when(isnan(c) | isnull(c), c)).alias(c) for c in uniDF.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniDF = uniDF.filter(col('BenchMark') <= 32.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT max(length(major_code)) FROM UNI').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniDF.write.format('jdbc').options(\n",
    "    url = 'jdbc:mysql://localhost:3306/equivalent_score',\n",
    "    driver = 'com.mysql.jdbc.Driver',\n",
    "    dbtable = \"University\",\n",
    "    user = dbUser,\n",
    "    password = password\n",
    ").mode('append').save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load to FactScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_18_22_df.createTempView('student1822')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_18_22_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student1822 = spark.sql(\"SELECT student_id as studentID, literature, mathematics as math, \\\n",
    "                         english, physics, chemistry, biology, history, geography, \\\n",
    "                         civic_education as civil, province_code as ProvinceCode, Year as year FROM student1822\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student1822 = student1822.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student1822.select('ProvinceCode').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinctProv = spark.sql(\"SELECT DISTINCT province_code FROM student1822\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinctProv.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student1822.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student1822.select([count(when(isnan(c) | isnull(c), c)).alias(c) for c in student1822.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student1822.select(max(col('literature'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student1822.write.format('jdbc').options(\n",
    "    url = 'jdbc:mysql://localhost:3306/equivalent_score',\n",
    "    driver = 'com.mysql.jdbc.Driver',\n",
    "    dbtable = \"FactScore\",\n",
    "    user = dbUser,\n",
    "    password = password\n",
    ").partitionBy('year').mode('append').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
